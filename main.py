# ============================================================
# ğŸ“¦ 1. ê¸°ë³¸ í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° (PyCharm ë²„ì „)
# ============================================================

import pandas as pd
import numpy as np
import requests
import datetime
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
import lightgbm as lgbm
import matplotlib.pyplot as plt

plt.rc('font', family='Malgun Gothic')  # ìœˆë„ìš° ê¸°ë³¸ í•œê¸€ í°íŠ¸


# ============================================================
# ğŸ“‚ 2. ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì¤‘ìš”!)
# ============================================================

BASE_PATH = r"C:\Users\james\PycharmProjects\finalproject\\"

FILE_NAME = BASE_PATH + "merged_apartment_with_coords.csv"
UNITS_FILE_NAME = BASE_PATH + "complex_units.csv"
RAW_KB_FILE_NAME = "weekly_apartment_jeonse_index_20251114.xlsx"
RAW_EXCEL_PATH = BASE_PATH + RAW_KB_FILE_NAME
OUTPUT_CSV_PATH = BASE_PATH + "proxy_data.csv"


# ============================================================
# ğŸ”‘ 3. ì¹´ì¹´ì˜¤ API í‚¤
# ============================================================

KAKAO_API_KEY = "c6943568281ead90d30d6c07d618eb7d"

def get_coords(address):
    url = f"https://dapi.kakao.com/v2/local/search/address.json?query={address}"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    response = requests.get(url, headers=headers).json()
    if not response.get('documents'):
        return None, None
    lon = response['documents'][0]['x']
    lat = response['documents'][0]['y']
    return float(lon), float(lat)


# ============================================================
# ğŸ“˜ 4. ë°ì´í„° ë¡œë“œ (ë¡œì»¬ CSV)
# ============================================================

print("ğŸ“‚ merged_apartment_with_coords.csv ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
df = pd.read_csv(FILE_NAME)
print("  â†’ ë¡œë“œ ì™„ë£Œ")

try:
    df_units = pd.read_csv(UNITS_FILE_NAME)
    df = pd.merge(df, df_units, on=['ì‹œêµ°êµ¬', 'ë‹¨ì§€ëª…'], how='left')
except:
    print("âš  complex_units.csv íŒŒì¼ì´ ì—†ì–´ ì´ì„¸ëŒ€ìˆ˜=0ìœ¼ë¡œ ì²˜ë¦¬ë¨")
    df['ì´ì„¸ëŒ€ìˆ˜'] = 0

print(f"ë°ì´í„° í¬ê¸°: {df.shape}")


# ============================================================
# ğŸ“¦ 5. proxy_data ìƒì„± (ì—‘ì…€ â†’ CSV)
# ============================================================

print("ğŸ“¦ proxy_data.csv ìƒì„± ì‹œì‘...")

df_raw = pd.read_excel(RAW_EXCEL_PATH, header=0)

if 'ì§€ì—­ëª…' in df_raw.columns:
    df_raw = df_raw.rename(columns={'ì§€ì—­ëª…': 'ì§€ì—­'})

df_seoul = df_raw[df_raw['ì§€ì—­'] == 'ì„œìš¸']

df_seoul_tall = pd.melt(
    df_seoul,
    id_vars=['ì§€ì—­'],
    var_name='ì£¼ì°¨',
    value_name='ì£¼ê°„ë³€ë™ë¥ '
)

df_seoul_tall['ì£¼ì°¨'] = pd.to_datetime(df_seoul_tall['ì£¼ì°¨'])
df_seoul_tall['ì£¼ê°„ë³€ë™ë¥ '] = pd.to_numeric(df_seoul_tall['ì£¼ê°„ë³€ë™ë¥ '], errors='ignore')

df_proxy = df_seoul_tall[['ì£¼ì°¨', 'ì£¼ê°„ë³€ë™ë¥ ']].sort_values('ì£¼ì°¨')

df_proxy.to_csv(OUTPUT_CSV_PATH, index=False)
print("  â†’ proxy_data.csv ìƒì„± ì™„ë£Œ")


# ============================================================
# ğŸ§¹ 6. ë³¸ê²©ì  ë°ì´í„° ì „ì²˜ë¦¬
# ============================================================

def get_haversine_distance(lat1, lon1, lat2, lon2):
    R = 6371
    dLat = np.radians(lat2 - lat1)
    dLon = np.radians(lon2 - lon1)
    lat1 = np.radians(lat1)
    lat2 = np.radians(lat2)
    a = np.sin(dLat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dLon/2)**2
    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1-a))


STATION_COORDS = {
    'ê±´ëŒ€ì…êµ¬ì—­': (37.540458, 127.069320), 'ê°•ë³€ì—­': (37.535102, 127.094761),
    'êµ¬ì˜ì—­': (37.537190, 127.086164), 'êµ°ìì—­': (37.557200, 127.079546),
    'ì•„ì°¨ì‚°ì—­': (37.551944, 127.089722), 'ê´‘ë‚˜ë£¨ì—­': (37.545291, 127.103485),
    'ìì–‘ì—­': (37.531667, 127.066667), 'ì–´ë¦°ì´ëŒ€ê³µì›ì—­': (37.547778, 127.074444),
    'ì¤‘ê³¡ì—­': (37.565833, 127.084167), 'ì„±ìˆ˜ì—­': (37.544583, 127.055972),
    'ëšì„¬ì—­': (37.547222, 127.047306), 'í•œì–‘ëŒ€ì—­': (37.555806, 127.043667),
    'ì™•ì‹­ë¦¬ì—­': (37.561194, 127.037444), 'ìƒì™•ì‹­ë¦¬ì—­': (37.564260, 127.029230),
    'ìš©ë§ˆì‚°ì—­': (37.573611, 127.086667), 'ì‚¬ê°€ì •ì—­': (37.580833, 127.088333),
    'ë©´ëª©ì—­': (37.588611, 127.087500),
}

KONKUK_UNIV = (37.5408, 127.0794)

def get_nearest_station_info(lat, lon):
    best_dist = np.inf
    best_name = None
    for name, (slat, slon) in STATION_COORDS.items():
        d = get_haversine_distance(lat, lon, slat, slon)
        if d < best_dist:
            best_dist = d
            best_name = name
    return best_dist, best_name


df_jeonse = df[df['ì „ì›”ì„¸êµ¬ë¶„'] == 'ì „ì„¸'].copy()

if 'ê³„ì•½êµ¬ë¶„' in df_jeonse.columns:
    df_jeonse = df_jeonse[df_jeonse['ê³„ì•½êµ¬ë¶„'] == 'ì‹ ê·œ']


target = 'ë³´ì¦ê¸ˆ(ë§Œì›)'
features = [
    'ì „ìš©ë©´ì (ã¡)', 'ì¸µ', 'ê±´ì¶•ë…„ë„', 'ì‹œêµ°êµ¬', 'ê³„ì•½ë…„ì›”',
    'ë‹¨ì§€ëª…', 'ì´ì„¸ëŒ€ìˆ˜', 'ìœ„ë„', 'ê²½ë„'
]

df_model = df_jeonse[features + [target]].dropna()

# ë‚ ì§œ ìƒì„±
s = df_model['ê³„ì•½ë…„ì›”'].astype(str)
dt_series = pd.to_datetime(s, format='%Y%m')

days_in_month = dt_series.dt.days_in_month
np.random.seed(42)
random_days = [np.random.randint(1, d + 1) for d in days_in_month]

df_model['ê³„ì•½ì¼_dt'] = dt_series + pd.to_timedelta(np.array(random_days)-1, unit='D')

# ì—°ì‹ ê³„ì‚°
df_model['ê±´ì¶•ë…„ë„_int'] = pd.to_numeric(df_model['ê±´ì¶•ë…„ë„'])
df_model['ì•„íŒŒíŠ¸ì—°ì‹'] = (df_model['ê³„ì•½ë…„ì›”']//100) - df_model['ê±´ì¶•ë…„ë„_int']

# ë¸Œëœë“œ ì•„íŒŒíŠ¸ ì—¬ë¶€
brand_list = ['ìì´','ë˜ë¯¸ì•ˆ','í‘¸ë¥´ì§€ì˜¤','íìŠ¤í…Œì´íŠ¸','ì•„ì´íŒŒí¬','eí¸í•œì„¸ìƒ','ë”ìƒµ','ë¡¯ë°ìºìŠ¬','SKVIEW','ìœ„ë¸Œ','ì•„ì´ì›']
brand_pattern = '|'.join(brand_list)
df_model['is_brand'] = df_model['ë‹¨ì§€ëª…'].str.contains(brand_pattern).astype(int)

# ê¸°ì¤€ê¸ˆë¦¬ ë§µ
rate_map = {
    202101: 0.50, 202102: 0.50, 202103: 0.50, 202104: 0.50, 202105: 0.50, 202106: 0.50,
    202107: 0.50, 202108: 0.75, 202109: 0.75, 202110: 0.75, 202111: 1.00, 202112: 1.00,
    202201: 1.25, 202202: 1.25, 202203: 1.25, 202204: 1.50, 202205: 1.75, 202206: 1.75,
    202207: 2.25, 202208: 2.50, 202209: 2.50, 202210: 3.00, 202211: 3.25, 202212: 3.25,
    202301: 3.50, 202302: 3.50, 202303: 3.50, 202304: 3.50, 202305: 3.50, 202306: 3.50,
    202307: 3.50, 202308: 3.50, 202309: 3.50, 202310: 3.50, 202311: 3.50, 202312: 3.50,
    202401: 3.50, 202402: 3.50, 202403: 3.50, 202404: 3.50, 202405: 3.50, 202406: 3.50,
    202407: 3.50, 202408: 3.50, 202409: 3.50, 202410: 3.25, 202411: 3.00, 202412: 3.00,
    202501: 3.00, 202502: 2.75, 202503: 2.75, 202504: 2.75, 202505: 2.50, 202506: 2.50,
    202507: 2.50, 202508: 2.50, 202509: 2.50, 202510: 2.50, 202511: 2.50, 202512: 2.50
}
# (ì—¬ê¸° í•™ìŠµ ì½”ë“œì—ì„œëŠ” ì‹¤ì œ rate_map ê·¸ëŒ€ë¡œ ë„£ì–´ì•¼ í•¨)

df_model['ê¸°ì¤€ê¸ˆë¦¬'] = df_model['ê³„ì•½ë…„ì›”'].map(rate_map)

df_model['ì´ì„¸ëŒ€ìˆ˜'] = df_model['ì´ì„¸ëŒ€ìˆ˜'].fillna(0)

# ì—­ ê±°ë¦¬ ê³„ì‚°
df_model['ì—­ê¹Œì§€ê±°ë¦¬(km)'] = df_model.apply(
    lambda r: get_nearest_station_info(r['ìœ„ë„'], r['ê²½ë„'])[0], axis=1
)
df_model['í•™êµê¹Œì§€ê±°ë¦¬(km)'] = df_model.apply(
    lambda r: get_haversine_distance(KONKUK_UNIV[0], KONKUK_UNIV[1], r['ìœ„ë„'], r['ê²½ë„']), axis=1
)

# í”„ë¡ì‹œ ë°ì´í„° ë³‘í•©
df_proxy = pd.read_csv(OUTPUT_CSV_PATH)
df_proxy['ì£¼ì°¨'] = pd.to_datetime(df_proxy['ì£¼ì°¨'])
df_proxy = df_proxy.sort_values('ì£¼ì°¨')

df_model = pd.merge_asof(
    df_model.sort_values('ê³„ì•½ì¼_dt'),
    df_proxy,
    left_on='ê³„ì•½ì¼_dt',
    right_on='ì£¼ì°¨',
    direction='backward'
)

df_model['ì£¼ê°„ë³€ë™ë¥ '] = df_model['ì£¼ê°„ë³€ë™ë¥ '].fillna(0)

final_features = [
    'ì „ìš©ë©´ì (ã¡)', 'ì¸µ', 'ì‹œêµ°êµ¬', 'ê³„ì•½ë…„ì›”', 'ì•„íŒŒíŠ¸ì—°ì‹',
    'is_brand', 'ê¸°ì¤€ê¸ˆë¦¬', 'ì´ì„¸ëŒ€ìˆ˜', 'ìœ„ë„', 'ê²½ë„',
    'ì—­ê¹Œì§€ê±°ë¦¬(km)', 'í•™êµê¹Œì§€ê±°ë¦¬(km)', 'ì£¼ê°„ë³€ë™ë¥ '
]

df_model = df_model[final_features + [target]]

df_model = pd.get_dummies(df_model, columns=['ì‹œêµ°êµ¬'], drop_first=True)

X = df_model.drop(target, axis=1)
y = df_model[target]


# ============================================================
# ğŸ§ª 7. ë°ì´í„° ë¶„í• 
# ============================================================

X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.2, random_state=42
)


# ============================================================
# ğŸš€ 8. ëª¨ë¸ í•™ìŠµ (XGBoost + LightGBM)
# ============================================================

xgb_model = XGBRegressor(
    n_estimators=2000, learning_rate=0.02, max_depth=6,
    subsample=0.8, colsample_bytree=0.8, random_state=42,
    n_jobs=-1, early_stopping_rounds=50
)
xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)

lgbm_model = LGBMRegressor(
    n_estimators=2000, learning_rate=0.02, max_depth=-1,
    subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1
)
lgbm_model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    callbacks=[lgbm.early_stopping(stopping_rounds=50, verbose=False)],
    eval_metric='rmse'
)


# ============================================================
# ğŸ  9. ì˜ˆì¸¡ (1ê°œ ìƒ˜í”Œ)
# ============================================================

new_house = pd.DataFrame(columns=X_train.columns)
new_house.loc[0] = 0

"""# ì˜ˆì¸¡ ì…ë ¥
input_area = 27
input_floor = 6
input_year_built = 2005
input_contract_date_str = "2025-01-20"
input_sigungu = "ì„œìš¸íŠ¹ë³„ì‹œ ê´‘ì§„êµ¬ í™”ì–‘ë™"
input_total_units = 312
input_is_brand = 0
input_address = "ì„œìš¸íŠ¹ë³„ì‹œ ê´‘ì§„êµ¬ ê´‘ë‚˜ë£¨ë¡œ 410"

contract_dt = pd.to_datetime(input_contract_date_str)
contract_ym = int(contract_dt.strftime('%Y%m'))

new_house['ì „ìš©ë©´ì (ã¡)'] = input_area
new_house['ì¸µ'] = input_floor
new_house['ê³„ì•½ë…„ì›”'] = contract_ym
new_house['ì•„íŒŒíŠ¸ì—°ì‹'] = (contract_ym//100) - input_year_built
new_house['ê¸°ì¤€ê¸ˆë¦¬'] = 3.5
new_house['ì´ì„¸ëŒ€ìˆ˜'] = input_total_units
new_house['is_brand'] = input_is_brand

# í”„ë¡ì‹œ ì ìš©
latest_proxy = df_proxy[df_proxy['ì£¼ì°¨'] <= contract_dt].iloc[-1]
new_house['ì£¼ê°„ë³€ë™ë¥ '] = latest_proxy['ì£¼ê°„ë³€ë™ë¥ ']

# ì¢Œí‘œ ê³„ì‚°
lon, lat = get_coords(input_address)
new_house['ê²½ë„'] = lon
new_house['ìœ„ë„'] = lat

dist, stn = get_nearest_station_info(lat, lon)
new_house['ì—­ê¹Œì§€ê±°ë¦¬(km)'] = dist

school_dist = get_haversine_distance(KONKUK_UNIV[0], KONKUK_UNIV[1], lat, lon)
new_house['í•™êµê¹Œì§€ê±°ë¦¬(km)'] = school_dist

# ì‹œêµ°êµ¬ ë”ë¯¸
dummy_name = 'ì‹œêµ°êµ¬_' + input_sigungu
if dummy_name in new_house.columns:
    new_house[dummy_name] = 1

pred_xgb = xgb_model.predict(new_house[X_train.columns])[0]
pred_lgbm = lgbm_model.predict(new_house[X_train.columns])[0]

print("\n===== ì˜ˆì¸¡ ê²°ê³¼ =====")
print(f"XGBoost: {pred_xgb:,.0f} ë§Œì›")
print(f"LightGBM: {pred_lgbm:,.0f} ë§Œì›")"""

import pickle

# ëª¨ë¸ ì €ì¥
with open("xgb_model.pkl", "wb") as f:
    pickle.dump(xgb_model, f)

with open("lgbm_model.pkl", "wb") as f:
    pickle.dump(lgbm_model, f)

pd.DataFrame({"columns": X_train.columns}).to_csv("feature_columns.csv", index=False)
print("feature_columns.csv ì €ì¥ ì™„ë£Œ")